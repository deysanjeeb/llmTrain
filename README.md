# llmTrain

Lightweight experiments exploring LLM-powered retrieval and fine-tuning. This repo includes Jupyter notebooks for data prep, LoRA tuning, and RAG-style experiments, plus a small Python demo (`rag.py`) that selects a relevant document via simple Jaccard similarity and generates a short recommendation using a local LLM served by Ollama.

## Overview

This project was built to prototype end-to-end workflows for:

- Preparing a small dataset for language-model experiments
- Running retrieval-augmented generation (RAG) baselines
- Exploring LoRA-style fine-tuning
- Capturing results and analysis in notebooks

The `rag.py` script demonstrates a minimal Retrieval + Generate loop:

1. Compute Jaccard similarity between the user input and a small in-memory corpus
2. Pick the best-matching document
3. Ask a local LLM (via Ollama) to craft a concise recommendation

## Repository Structure

- `rag.py` — Minimal RAG demo using Jaccard similarity + Ollama generation
- `EECS230_Final.ipynb` — Main notebook for the project write-up/workflow
- `EECS230_Final_Experiments_(RAG).ipynb` — RAG experiments and evaluation
- `EECS230_Final_DataPrep.ipynb` — Data cleaning/preparation steps
- `EECS230_Final_Results.ipynb` — Aggregated results and discussion
- `lora_tuning.ipynb` — LoRA-style fine-tuning exploration

## Quickstart

### Prerequisites

- Python 3.9+
- `pip` (or `uv`/`pipx`)
- [Ollama](https://github.com/ollama/ollama) installed and running locally
  - Pull a compatible model, e.g.: `ollama pull mistral`

### Setup

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
pip install requests jupyter
```

### Run the demo

1. Make sure the Ollama server is running locally (default: `http://localhost:11434`).
2. Ensure you have the `mistral` model pulled: `ollama pull mistral`.
3. Run the script:

```bash
python rag.py
```

You will be prompted for a short description (e.g., an activity you enjoy). The script will select a relevant suggestion from a small corpus and stream a concise recommendation generated by the local model.

## Notebooks

You can open and run any of the included notebooks in Jupyter:

```bash
jupyter notebook
```

Suggested order if you’re exploring the full workflow:

1. `EECS230_Final_DataPrep.ipynb`
2. `EECS230_Final_Experiments_(RAG).ipynb`
3. `lora_tuning.ipynb`
4. `EECS230_Final_Results.ipynb`
5. `EECS230_Final.ipynb`

Note: The fine-tuning and RAG notebooks may expect local data or environment settings (e.g., model names, paths). Adjust cells as needed for your machine.

## Configuration

- Ollama endpoint: Update `url` in `rag.py` if your server is not on `http://localhost:11434`.
- Model: Change `data["model"]` in `rag.py` to any model you have locally (e.g., `llama3`, `mistral`, etc.).
- Corpus: Edit the `corpus_of_documents` list in `rag.py` to use your own domain content.

## Troubleshooting

- Ollama connection errors: Verify the server is running and reachable at the configured URL. Try `ollama list` and `curl http://localhost:11434`.
- Model not found: Pull the model first, e.g., `ollama pull mistral`.
- Unicode/encoding issues in terminal: Ensure your shell supports UTF-8.

## Roadmap / Ideas

- Replace Jaccard similarity with embeddings-based retrieval
- Add a `requirements.txt` and environment reproducibility via `uv` or `pip-tools`
- Include small sample datasets for notebook runs
- Package the demo as a CLI with corpus management

## Acknowledgements

- Ollama API reference: https://github.com/ollama/ollama
- LoRA and RAG concepts inspired by common open-source examples and academic literature
