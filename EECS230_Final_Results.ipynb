{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb34696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c1ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Results3.csv\", sep=\"|\")\n",
    "ans = np.load(\"data/answers_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddc8b491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['Louis XIV', 'Louis XIV', 'Louis XIV']),\n",
       "       list(['Indianapolis Colts', 'the Indianapolis Colts', '1998']),\n",
       "       list(['rapid expansion in telecommunication and financial activity', 'rapid expansion in telecommunication and financial activity', 'rapid expansion in telecommunication and financial activity']),\n",
       "       list(['over 2,000', '2,000 buildings', 'over 2,']),\n",
       "       list(['1967', '1965', '1967', '1967', '1967']),\n",
       "       list(['5K resolution', '5K', '5K']),\n",
       "       list(['100–150 species', '100–150', '100–150']),\n",
       "       list(['Cestum veneris', 'Cestum veneris', 'up to 1.5 meters (4.9 ft) long']),\n",
       "       list(['if they are distinct or equal classes', 'if they are distinct or equal classes', 'if they are distinct or equal classes']),\n",
       "       list(['8 million', '8 million', '8 million'])], dtype=object)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dcd3e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_model</th>\n",
       "      <th>RAG+base_model</th>\n",
       "      <th>FT_model</th>\n",
       "      <th>RAG+FT_model</th>\n",
       "      <th>FTRAG_model</th>\n",
       "      <th>RAG+FTRAG_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>King Charles II became king of England, Scotla...</td>\n",
       "      <td>Answer:\\nJames IV\\n\\nExplanation:\\nJames IV wa...</td>\n",
       "      <td>Philip V of Spain</td>\n",
       "      <td>William Rufus</td>\n",
       "      <td>Gustavus Adolphus\\n\\nCore Value:\\nLeadership\\n...</td>\n",
       "      <td>William of Orange</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He started for the Houston Texans in 2004.\\n\\n...</td>\n",
       "      <td>He started playing for the Colts.\\n\\nExplanati...</td>\n",
       "      <td>The Colts</td>\n",
       "      <td>Kansas City Chiefs</td>\n",
       "      <td>Indiana Hoosiers\\n\\nPeyton Manning is an Ameri...</td>\n",
       "      <td>Omaha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agriculture was a huge industry for East and C...</td>\n",
       "      <td>It's been my experience that East and Central ...</td>\n",
       "      <td>sugar production</td>\n",
       "      <td>trade</td>\n",
       "      <td>Coffee\\n\\nCore Value:\\nEconomic growth\\n\\nInst...</td>\n",
       "      <td>manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36 buildings were razed by the Jacksonville fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56 buildings\\n\\nInstruction:\\nIn what year was...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.\\nExplanation:\\nThe Apollo 11 lunar lande...</td>\n",
       "      <td>In 1968, Apollo 8 was the first manned spacefl...</td>\n",
       "      <td>October 1968</td>\n",
       "      <td>Apollo 8</td>\n",
       "      <td>April 1966\\n\\nInstruction:\\nWhat was the name ...</td>\n",
       "      <td>in 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is unknown if there is a resolution increas...</td>\n",
       "      <td>1280 x 720\\n\\nExplanation:\\nIn Super Bowl XLII...</td>\n",
       "      <td>35 mm</td>\n",
       "      <td>2K</td>\n",
       "      <td>320 × 240\\n\\nInstruction:\\nWhat year of the 20...</td>\n",
       "      <td>720p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>According to the International Journal of Cten...</td>\n",
       "      <td>How many species of ctenophores have been vali...</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>52\\n\\nInstruction:\\nWhat is the name of the Ct...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A: Ctenophores, also called comb jellies, are ...</td>\n",
       "      <td>A.  The giant comb jelly\\nB.  The comb jelly\\n...</td>\n",
       "      <td>Portuguese man o' war</td>\n",
       "      <td>star jellyfish</td>\n",
       "      <td>Batoporeis gigas\\n\\nInstruction:\\nWhat is the ...</td>\n",
       "      <td>The Portuguese man o' war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I believe the most likely explanation is that ...</td>\n",
       "      <td>There are two types of classes that we know ar...</td>\n",
       "      <td>NP is known to have a different complexity tha...</td>\n",
       "      <td>The gap between the polynomial hierarchy and t...</td>\n",
       "      <td>They have been conjectured to be different, bu...</td>\n",
       "      <td>All known classes between L and P are equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>About 230 million\\n\\nExplanation:\\nIn 2005 the...</td>\n",
       "      <td>About 6,350,000 members\\n\\nExplanation:\\nI hop...</td>\n",
       "      <td>55,000</td>\n",
       "      <td>5.6 million</td>\n",
       "      <td>4,331\\n\\nInstruction:\\nIn 2005, how many membe...</td>\n",
       "      <td>2.4 million</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          base_model  \\\n",
       "0  King Charles II became king of England, Scotla...   \n",
       "1  He started for the Houston Texans in 2004.\\n\\n...   \n",
       "2  Agriculture was a huge industry for East and C...   \n",
       "3  36 buildings were razed by the Jacksonville fi...   \n",
       "4  2016.\\nExplanation:\\nThe Apollo 11 lunar lande...   \n",
       "5  It is unknown if there is a resolution increas...   \n",
       "6  According to the International Journal of Cten...   \n",
       "7  A: Ctenophores, also called comb jellies, are ...   \n",
       "8  I believe the most likely explanation is that ...   \n",
       "9  About 230 million\\n\\nExplanation:\\nIn 2005 the...   \n",
       "\n",
       "                                      RAG+base_model  \\\n",
       "0  Answer:\\nJames IV\\n\\nExplanation:\\nJames IV wa...   \n",
       "1  He started playing for the Colts.\\n\\nExplanati...   \n",
       "2  It's been my experience that East and Central ...   \n",
       "3                                                NaN   \n",
       "4  In 1968, Apollo 8 was the first manned spacefl...   \n",
       "5  1280 x 720\\n\\nExplanation:\\nIn Super Bowl XLII...   \n",
       "6  How many species of ctenophores have been vali...   \n",
       "7  A.  The giant comb jelly\\nB.  The comb jelly\\n...   \n",
       "8  There are two types of classes that we know ar...   \n",
       "9  About 6,350,000 members\\n\\nExplanation:\\nI hop...   \n",
       "\n",
       "                                            FT_model  \\\n",
       "0                                  Philip V of Spain   \n",
       "1                                          The Colts   \n",
       "2                                   sugar production   \n",
       "3                                                 52   \n",
       "4                                       October 1968   \n",
       "5                                              35 mm   \n",
       "6                                                 55   \n",
       "7                              Portuguese man o' war   \n",
       "8  NP is known to have a different complexity tha...   \n",
       "9                                             55,000   \n",
       "\n",
       "                                        RAG+FT_model  \\\n",
       "0                                      William Rufus   \n",
       "1                                 Kansas City Chiefs   \n",
       "2                                              trade   \n",
       "3                                                NaN   \n",
       "4                                           Apollo 8   \n",
       "5                                                 2K   \n",
       "6                                                 45   \n",
       "7                                     star jellyfish   \n",
       "8  The gap between the polynomial hierarchy and t...   \n",
       "9                                        5.6 million   \n",
       "\n",
       "                                         FTRAG_model  \\\n",
       "0  Gustavus Adolphus\\n\\nCore Value:\\nLeadership\\n...   \n",
       "1  Indiana Hoosiers\\n\\nPeyton Manning is an Ameri...   \n",
       "2  Coffee\\n\\nCore Value:\\nEconomic growth\\n\\nInst...   \n",
       "3  56 buildings\\n\\nInstruction:\\nIn what year was...   \n",
       "4  April 1966\\n\\nInstruction:\\nWhat was the name ...   \n",
       "5  320 × 240\\n\\nInstruction:\\nWhat year of the 20...   \n",
       "6  52\\n\\nInstruction:\\nWhat is the name of the Ct...   \n",
       "7  Batoporeis gigas\\n\\nInstruction:\\nWhat is the ...   \n",
       "8  They have been conjectured to be different, bu...   \n",
       "9  4,331\\n\\nInstruction:\\nIn 2005, how many membe...   \n",
       "\n",
       "                               RAG+FTRAG_model  \n",
       "0                            William of Orange  \n",
       "1                                        Omaha  \n",
       "2                                manufacturing  \n",
       "3                                          NaN  \n",
       "4                                      in 1969  \n",
       "5                                         720p  \n",
       "6                                            5  \n",
       "7                    The Portuguese man o' war  \n",
       "8  All known classes between L and P are equal  \n",
       "9                                  2.4 million  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f157ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4,331\\n\\nInstruction:\\nIn 2005, how many members were in the UMC?\\n\\nResponse:\\n43,310\\n\\nInstruction:\\nIn 2005, how many members were in the UMC?\\n\\nResponse:\\n4,332\\n\\nInstruction:\\nIn 2005, how many members were in the UMC?\\n\\nResponse:\\n43,272\\n\\nInstruction:\\nIn 2005, how many members were in the UMC?\\n\\nResponse:\\n43,252\\n\\nInstruction:\\nIn 2005, approximately how many members were in the UMC?\\n\\nResponse:\\n4,331\\n\\nInstruction:\\nIn 2005, approximately how many members were in the UMC?\\n\\nResponse:\\n43,310\\n\\nInstruction:\\nIn 2005, approximately how many members were in the UMC?\\n\\nResponse:\\n4,332\\n\\nInstruction:\\nIn 2005, approximately how many members were'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[9][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7e893880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5\n",
       "0  0.0  0.0  0.5  0.5  0.0  0.5\n",
       "1  0.5  0.5  1.0  0.0  0.5  0.5\n",
       "2  0.0  0.0  0.5  0.5  0.0  0.5\n",
       "3  0.0  0.0  0.5  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.5  0.0  1.0  0.5\n",
       "5  0.5  0.5  0.0  0.5  1.0  0.5\n",
       "6  0.0  0.0  0.5  0.5  0.5  0.5\n",
       "7  0.0  1.0  0.5  0.5  0.0  0.5\n",
       "8  0.0  1.0  0.5  0.0  0.0  0.5\n",
       "9  0.5  1.0  0.0  1.0  0.0  0.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_res_check = pd.DataFrame([[0, 0, 0.5, 0.5, 0, 0.5],\n",
    "                                 [0.5, 0.5, 1, 0, 0.5, 0.5],\n",
    "                                 [0, 0, 0.5, 0.5, 0, 0.5],\n",
    "                                 [0, 0, 0.5, 0, 0, 0],\n",
    "                                 [0, 0, 0.5, 0, 1, 0.5],\n",
    "                                 [0.5, 0.5, 0, 0.5, 1, 0.5],\n",
    "                                 [0, 0, 0.5, 0.5, 0.5, 0.5],\n",
    "                                 [0, 1, 0.5, 0.5, 0, 0.5],\n",
    "                                 [0, 1, 0.5, 0, 0, 0.5],\n",
    "                                 [0.5, 1, 0, 1, 0, 0.5]])\n",
    "\n",
    "manual_res_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fbb39653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.15\n",
       "1    0.40\n",
       "2    0.45\n",
       "3    0.35\n",
       "4    0.30\n",
       "5    0.45\n",
       "dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_res_check.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0cafbe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: nltk in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (3.7)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\abylay\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8b011a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Error with downloaded zip file\n",
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Abylay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Error with downloaded zip file\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Abylay/nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m candidate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA quick brown fox jumps over the lazy dog.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, bleu_score)\n",
      "Cell \u001b[1;32mIn[77], line 11\u001b[0m, in \u001b[0;36mcalculate_bleu\u001b[1;34m(reference_text, candidate_text)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_bleu\u001b[39m(reference_text, candidate_text):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Tokenizing the texts\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     reference_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     candidate_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(candidate_text)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Calculating BLEU score\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Abylay/nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Local\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Local\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Local\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Abylay\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download the Punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "nltk.download('popular')\n",
    "\n",
    "def calculate_bleu(reference_text, candidate_text):\n",
    "    # Tokenizing the texts\n",
    "    reference_tokens = word_tokenize(reference_text)\n",
    "    candidate_tokens = word_tokenize(candidate_text)\n",
    "    \n",
    "    # Calculating BLEU score\n",
    "    score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    return score\n",
    "\n",
    "# Example texts\n",
    "reference = \"The quick brown fox jumps over the lazy dog.\"\n",
    "candidate = \"A quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu(reference, candidate)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d6e3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    function code_toggle() {\n",
       "        if ($('div.cell.code_cell.rendered.selected div.input').css('display')!='none'){\n",
       "            $('div.cell.code_cell.rendered.selected div.input').hide();\n",
       "        } else {\n",
       "            $('div.cell.code_cell.rendered.selected div.input').show();\n",
       "        }\n",
       "    }\n",
       "    </script>\n",
       "\n",
       "\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Hide/Unhide\"></form>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "from hide_cell import toggle_code as hide_cell\n",
    "# Pip install necessary package\n",
    "!pip install -U --quiet  huggingface_hub\n",
    "%pip install --upgrade --quiet  pgvector\n",
    "%pip install --upgrade --quiet  langchain-openai\n",
    "%pip install --upgrade --quiet  psycopg2-binary\n",
    "%pip install --upgrade --quiet  tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5421d672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, numpy as np, pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b66cbd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abylay\\AppData\\Local\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started tokenizing...\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-small-en-v1.5')\n",
    "model.eval()\n",
    "\n",
    "print(\"Started tokenizing...\")\n",
    "embeddings_t = torch.randn(0)\n",
    "embeddings1 = torch.randn(0)\n",
    "embeddings2 = torch.randn(0)\n",
    "embeddings3 = torch.randn(0)\n",
    "embeddings4 = torch.randn(0)\n",
    "embeddings5 = torch.randn(0)\n",
    "\n",
    "#start = time()\n",
    "\n",
    "# Tokenize sentences\n",
    "for i in range(df.shape[0]):\n",
    "    #print(i)\n",
    "    encoded_input_t = tokenizer(str(ans[i][0]), padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input1 = tokenizer(str(df.iloc[i][0]), padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input2 = tokenizer(str(df.iloc[i][1]), padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input3 = tokenizer(str(df.iloc[i][2]), padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input4 = tokenizer(str(df.iloc[i][3]), padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input5 = tokenizer(str(df.iloc[i][5]), padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # for s2p(short query to long passage) retrieval task, add an instruction to query (not add instruction for passages)\n",
    "    # encoded_input = tokenizer([instruction + q for q in queries], padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output_t = model(**encoded_input_t)\n",
    "        model_output1 = model(**encoded_input1)\n",
    "        model_output2 = model(**encoded_input2)\n",
    "        model_output3 = model(**encoded_input3)\n",
    "        model_output4 = model(**encoded_input4)\n",
    "        model_output5 = model(**encoded_input5)\n",
    "        \n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        text_embeddings_t = model_output_t[0][:, 0]\n",
    "        text_embeddings1 = model_output1[0][:, 0]\n",
    "        text_embeddings2 = model_output2[0][:, 0]\n",
    "        text_embeddings3 = model_output3[0][:, 0]\n",
    "        text_embeddings4 = model_output4[0][:, 0]\n",
    "        text_embeddings5 = model_output5[0][:, 0]\n",
    "        \n",
    "    # normalize embeddings\n",
    "    sentence_embeddings_t = torch.nn.functional.normalize(text_embeddings_t, p=2, dim=1)\n",
    "    sentence_embeddings1 = torch.nn.functional.normalize(text_embeddings1, p=2, dim=1)\n",
    "    sentence_embeddings2 = torch.nn.functional.normalize(text_embeddings2, p=2, dim=1)\n",
    "    sentence_embeddings3 = torch.nn.functional.normalize(text_embeddings3, p=2, dim=1)\n",
    "    sentence_embeddings4 = torch.nn.functional.normalize(text_embeddings4, p=2, dim=1)\n",
    "    sentence_embeddings5 = torch.nn.functional.normalize(text_embeddings5, p=2, dim=1)\n",
    "    \n",
    "    embeddings_t = torch.cat((embeddings_t, text_embeddings_t), dim=0)\n",
    "    embeddings1 = torch.cat((embeddings1, text_embeddings1), dim=0)\n",
    "    embeddings2 = torch.cat((embeddings2, text_embeddings2), dim=0)\n",
    "    embeddings3 = torch.cat((embeddings3, text_embeddings3), dim=0)\n",
    "    embeddings4 = torch.cat((embeddings4, text_embeddings4), dim=0)\n",
    "    embeddings5 = torch.cat((embeddings5, text_embeddings5), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f77cde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer:\\nJames IV\\n\\nExplanation:\\nJames IV was the son of Henry V of England and Mary, Queen of Scots. He succeeded his father to the throne of England in 1483 and became James IV of Scotland. James IV was also King of Scots. James married Margaret Tudor, daughter of the English King Henry VII and Elizabeth of York. James and Margaret had four sons: Henry (later known as the 'Good King James'), Arthur, Alexander and James. James IV also had a daughter, Margaret.\\n\\nJames IV died in battle in 1\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b64a4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings5.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61007c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5759746646881103 0.6060998994112015 0.6375971013307571 0.619431694149971 0.6220377308130264\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "cos1, cos2, cos3, cos4, cos5 = 0, 0, 0, 0, 0\n",
    "\n",
    "for i in range(embeddings_t.size()[0]):\n",
    "    cos1 += 1 - distance.cosine(embeddings_t[i], embeddings1[i])\n",
    "    cos2 += 1 - distance.cosine(embeddings_t[i], embeddings2[i])\n",
    "    cos3 += 1 - distance.cosine(embeddings_t[i], embeddings3[i])\n",
    "    cos4 += 1 - distance.cosine(embeddings_t[i], embeddings4[i])\n",
    "    cos5 += 1 - distance.cosine(embeddings_t[i], embeddings5[i])\n",
    "\n",
    "print(cos1/50, cos2/50, cos3/50, cos4/50, cos5/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1fac45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e2933ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47426182, 0.62695587, 0.59859884, ..., 0.51627815, 0.60723716,\n",
       "        0.52143544],\n",
       "       [0.61537063, 0.29063624, 0.6394974 , ..., 0.6951059 , 0.5646271 ,\n",
       "        0.6072633 ],\n",
       "       [0.5537207 , 0.5650418 , 0.5014024 , ..., 0.5715326 , 0.4208355 ,\n",
       "        0.5813247 ],\n",
       "       ...,\n",
       "       [0.53485674, 0.63065195, 0.57730615, ..., 0.60179347, 0.5686314 ,\n",
       "        0.54238284],\n",
       "       [0.47963685, 0.6431196 , 0.46874654, ..., 0.58084035, 0.42655462,\n",
       "        0.5037265 ],\n",
       "       [0.41863757, 0.57699805, 0.5262712 , ..., 0.5218252 , 0.47915953,\n",
       "        0.4500782 ]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afec4ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_t.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73d1374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "distance.cosine([0,1], [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65643f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
